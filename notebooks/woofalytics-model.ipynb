{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0a42bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import glob\n",
    "import tqdm.auto as tqdm\n",
    "\n",
    "import pathlib\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "import torchaudio.compliance\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import collections\n",
    "\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a0c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WoofalyticsDataset(Dataset):\n",
    "    def __init__(self, data_dir: str, labels_tsv: str):\n",
    "        wave_files = list(glob.glob(f\"{data_dir}/*.wav\"))\n",
    "        labels_tsv = self._load_labels(labels_tsv)\n",
    "        data = []\n",
    "        times = []\n",
    "        labels = []\n",
    "        for wave_file in tqdm.tqdm(wave_files):\n",
    "            wave_form = self._load_wave(wave_file)\n",
    "            file_id = pathlib.Path(wave_file).stem\n",
    "            feats = self._extract_features(wave_form)\n",
    "            print(\"waveform shape\", wave_form.shape)\n",
    "            print(\"feats shape\", feats.shape)\n",
    "            file_labels = []\n",
    "            if file_id not in labels_tsv:\n",
    "                print(f\"File {file_id} does not have any labels.\")\n",
    "            else:\n",
    "                file_labels = [(start, start+duration) for start, duration in labels_tsv[file_id]]\n",
    "            \n",
    "            win_len = 6\n",
    "            overlap = 3\n",
    "            res = self._extract_overlapping_sections(feats, win_len, overlap)\n",
    "            print(\"res len\", len(res))\n",
    "            for idx, item in enumerate(res):\n",
    "                start = idx*(overlap/100.)\n",
    "                end = start + (win_len/100.)\n",
    "                lbl = self._is_range_within_any(file_labels, (start, end))\n",
    "                data.append(item)\n",
    "                labels.append(lbl)\n",
    "                times.append((file_id, start, end))\n",
    "                \n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.times = times\n",
    "        assert len(self.data) == len(self.labels) == len(self.times)\n",
    "\n",
    "    def _load_labels(self, labels_tsv: str) -> Dict[str, List[Tuple[float, float]]]:\n",
    "        output = {}\n",
    "        with open(labels_tsv, \"r\") as file_handle:\n",
    "            for idx, line in enumerate(file_handle):\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 3:\n",
    "                    print(f\"Invalid line: {idx+1}: {line.strip()}: {parts}\")\n",
    "                else:\n",
    "                    filename, start, end = parts\n",
    "                    start, end = float(start), float(end)\n",
    "                    if filename not in output:\n",
    "                        output[filename] = []\n",
    "                    \n",
    "                    output[filename].append((start, end))\n",
    "        return output\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.data[idx].flatten().unsqueeze(0), \n",
    "                torch.tensor([[1.] if self.labels[idx] else [0.]]), \n",
    "                self.times[idx])\n",
    "    \n",
    "    def _is_range_within_any(self, input_ranges, given_range):\n",
    "        \"\"\"\n",
    "        Check if a given range falls within any of the input ranges.\n",
    "\n",
    "        Args:\n",
    "            input_ranges (list): List of tuples representing ranges in the format [(start1, end1), (start2, end2), ...].\n",
    "            given_range (tuple): The range to check in the format (start, end).\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the given range falls within any of the input ranges, False otherwise.\n",
    "        \"\"\"\n",
    "        start, end = given_range\n",
    "\n",
    "        for input_range in input_ranges:\n",
    "            input_start, input_end = input_range\n",
    "            if input_start <= start <= input_end or input_start <= end <= input_end:\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "    def _extract_features(self, wave_form):       \n",
    "        mel_spectrogram = torchaudio.compliance.kaldi.fbank(num_mel_bins=80, \n",
    "                                                            frame_length=25, \n",
    "                                                            frame_shift=10,\n",
    "                                                            waveform=wave_form)\n",
    "        return mel_spectrogram\n",
    "    \n",
    "    def _extract_overlapping_sections(self, tensor, section_length, overlap):\n",
    "        \"\"\"\n",
    "        Extract overlapping sections from a torch tensor.\n",
    "\n",
    "        Args:\n",
    "            tensor (torch.Tensor): Input tensor of shape TxD.\n",
    "            section_length (int): Length of each section to be extracted.\n",
    "            overlap (int): Number of elements to overlap between consecutive sections.\n",
    "\n",
    "        Returns:\n",
    "            List of overlapping sections, each with shape section_lengthxD.\n",
    "        \"\"\"\n",
    "        print(\"tensor size\", tensor.size())\n",
    "        T, D = tensor.size()\n",
    "        sections = []\n",
    "\n",
    "        # Ensure the section_length is not greater than the input tensor size\n",
    "        section_length = min(section_length, T)\n",
    "\n",
    "        # Start extracting sections\n",
    "        start_idx = 0\n",
    "        while start_idx + section_length <= T:\n",
    "            end_idx = start_idx + section_length\n",
    "            section = tensor[start_idx:end_idx]\n",
    "            sections.append(section)\n",
    "            start_idx += section_length - overlap\n",
    "\n",
    "        return sections\n",
    "\n",
    "\n",
    "    def _load_wave(self, filename: str):\n",
    "        waveform, sample_rate = torchaudio.load(filename)\n",
    "        #assert sample_rate == 44_100\n",
    "        #return waveform\n",
    "        resample_rate = 16_000\n",
    "        resampler = T.Resample(sample_rate, resample_rate, dtype=waveform.dtype)\n",
    "        resampled_waveform = resampler(waveform)\n",
    "        return resampled_waveform\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bccd94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = WoofalyticsDataset(data_dir=\"../data/train/\", labels_tsv=\"../data/labels.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca1344e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = WoofGuardDataset(data_dir=\"data/dev/\", labels_tsv=\"data/labels.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39132a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = WoofGuardDataset(data_dir=\"data/test/\", labels_tsv=\"data/labels.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733bf91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WoofClassifier(pl.LightningModule):\n",
    "    def __init__(self, input_size):\n",
    "        super(WoofClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.output_layer = nn.Linear(32, 1)\n",
    "        \n",
    "        self.validation_step_outputs = collections.defaultdict(list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.output_layer(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, targets, times = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = F.binary_cross_entropy(outputs, targets)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, targets, times = batch\n",
    "        outputs = self(inputs)\n",
    "        \n",
    "        loss = F.binary_cross_entropy(outputs, targets)\n",
    "        self.log('val_loss', loss)\n",
    "        \n",
    "        acc = self.compute_accuracy(outputs, targets)\n",
    "        self.log('val_acc', acc)\n",
    "        \n",
    "        self.validation_step_outputs[\"loss\"].append(loss)\n",
    "        self.validation_step_outputs[\"acc\"].append(acc)\n",
    "        return loss\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = torch.stack([x for x in self.validation_step_outputs[\"loss\"]]).mean()\n",
    "        avg_acc = torch.stack([x for x in self.validation_step_outputs[\"acc\"]]).mean()\n",
    "        self.validation_step_outputs.clear()\n",
    "        return {'val_loss': avg_loss, 'val_acc': avg_acc}    \n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "    def compute_accuracy(self, predictions, labels):\n",
    "        # Convert predictions to binary (0 or 1) based on threshold 0.5\n",
    "        binary_predictions = (predictions >= 0.5).float()\n",
    "\n",
    "        # Compare binary predictions with the correct labels\n",
    "        correct_predictions = torch.eq(binary_predictions, labels.float())\n",
    "\n",
    "        # Calculate accuracy as the percentage of correct predictions\n",
    "        accuracy = torch.mean(correct_predictions.float())\n",
    "\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246683d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 480\n",
    "model = WoofClassifier(input_size)\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_acc', mode='max', patience=5, strict=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=32)\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = pl.Trainer(max_epochs=100, callbacks=[early_stopping_callback])  # You can modify max_epochs and gpus according to your requirements\n",
    "tb_logger = pl.loggers.TensorBoardLogger('logs/')  # Logs will be saved in the 'logs' directory\n",
    "\n",
    "# Train the model using DataLoader\n",
    "trainer.fit(model, train_loader, dev_loader)\n",
    "\n",
    "# tb_logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13117570",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model = torch.jit.trace(model, train_dataset[0][0])\n",
    "scripted_model = torch.jit.script(model)\n",
    "\n",
    "# Save the JIT-compiled model\n",
    "torch.jit.save(traced_model, \"traced_model.pt\")\n",
    "torch.jit.save(scripted_model, \"scripted_model.pt\")\n",
    "\n",
    "loaded_traced_model = torch.jit.load(\"traced_model.pt\")\n",
    "loaded_scripted_model = torch.jit.load(\"scripted_model.pt\")\n",
    "\n",
    "print(train_dataset[0][0].shape)\n",
    "output = loaded_traced_model(train_dataset[0][0])\n",
    "print(output)\n",
    "output = loaded_scripted_model(train_dataset[0][0])\n",
    "print(output)\n",
    "\n",
    "print(model(train_dataset[0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2b5a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def find_best_f1_threshold(predictions, labels):\n",
    "    #thresholds = sorted(set(predictions))  # Unique and sorted prediction values as potential thresholds\n",
    "    thresholds = np.arange(0,1, 0.01)\n",
    "    best_f1 = 0.0\n",
    "    best_threshold = 0.0\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        binary_predictions = [1 if pred >= threshold else 0 for pred in predictions]\n",
    "        f1 = f1_score(labels, binary_predictions)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "\n",
    "    return best_threshold, best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33363582",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "labels = []\n",
    "for item in dev_dataset:\n",
    "    preds.append(model(item[0])[0].detach().numpy()[0])\n",
    "    labels.append(item[1][0].numpy()[0])\n",
    "    \n",
    "threshold, f1 = find_best_f1_threshold(preds, labels)\n",
    "print(threshold, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b4fac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "indexes = collections.defaultdict(list)\n",
    "for idx, (data, label, times) in enumerate(test_dataset):\n",
    "    if model(data)>= threshold:\n",
    "        print(idx, times, model(data)[0][0])\n",
    "        indexes[times[0]].append((times[1], times[2]))\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5147727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_drop_segments(segments, threshold):\n",
    "    # Step 1: Sort segments based on their start times\n",
    "    sorted_segments = sorted(segments, key=lambda x: x[0])\n",
    "\n",
    "    # Step 2: Merge neighboring segments\n",
    "    merged_segments = []\n",
    "    current_segment = sorted_segments[0]\n",
    "\n",
    "    for segment in sorted_segments[1:]:\n",
    "        if segment[0] <= current_segment[1]:  # Overlapping or adjacent\n",
    "            current_segment = (current_segment[0], max(current_segment[1], segment[1]))\n",
    "        else:\n",
    "            merged_segments.append(current_segment)\n",
    "            current_segment = segment\n",
    "\n",
    "    merged_segments.append(current_segment)  # Append the last segment\n",
    "\n",
    "    # Step 3: Drop segments with duration less than the threshold\n",
    "    final_segments = [segment for segment in merged_segments if segment[1] - segment[0] >= threshold]\n",
    "\n",
    "    return final_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586b9393",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn=\"1693562020483807935\"\n",
    "merge_and_drop_segments(indexes[fn], 0.095)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732d4456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "def audio(fn, start_seconds=0, end_seconds=0):\n",
    "    SPEECH_WAVEFORM, SAMPLE_RATE = torchaudio.load(f\"data/test/{fn}.wav\")\n",
    "    if start_seconds == 0 and end_seconds == 0:\n",
    "        return Audio(SPEECH_WAVEFORM.numpy(), rate=SAMPLE_RATE)        \n",
    "    else:\n",
    "        s = int(start_seconds*SAMPLE_RATE)\n",
    "        e = int(end_seconds*SAMPLE_RATE)\n",
    "        assert e>s\n",
    "        print(s, e, SPEECH_WAVEFORM.shape)\n",
    "        return Audio(SPEECH_WAVEFORM.numpy()[:,s:e], rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809f8287",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7232ce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "audios = []\n",
    "merged = merge_and_drop_segments(indexes[fn], 0.095)\n",
    "for item in merged:\n",
    "    audios.append(audio(fn, item[0], item[1]))\n",
    "print(len(audios))\n",
    "for item in merged:\n",
    "    print(item, item[1]-item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b68be31",
   "metadata": {},
   "outputs": [],
   "source": [
    "audios[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d4b9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "audios[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb0b76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sample_rate = torchaudio.load(\"sample.wav\", normalize=False)\n",
    "waveform.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33247e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0][0].size()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e2c700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_file(wav_filename, model, threshold):\n",
    "    def load_wav(filename):\n",
    "        waveform, sample_rate = torchaudio.load(filename)\n",
    "        resample_rate = 16_000\n",
    "        resampler = T.Resample(sample_rate, resample_rate, dtype=waveform.dtype)\n",
    "        resampled_waveform = resampler(waveform)\n",
    "        return resampled_waveform\n",
    "    def extract_features(wave_form):       \n",
    "        mel_spectrogram = torchaudio.compliance.kaldi.fbank(num_mel_bins=80, \n",
    "                                                            frame_length=25, \n",
    "                                                            frame_shift=10,\n",
    "                                                            waveform=wave_form)\n",
    "        return mel_spectrogram\n",
    "    def extract_overlapping_sections(tensor, section_length, overlap):\n",
    "        T, D = tensor.size()\n",
    "        sections = []\n",
    "\n",
    "        # Ensure the section_length is not greater than the input tensor size\n",
    "        section_length = min(section_length, T)\n",
    "\n",
    "        # Start extracting sections\n",
    "        start_idx = 0\n",
    "        while start_idx + section_length <= T:\n",
    "            end_idx = start_idx + section_length\n",
    "            section = tensor[start_idx:end_idx]\n",
    "            sections.append(section)\n",
    "            start_idx += section_length - overlap\n",
    "\n",
    "        return sections\n",
    "    \n",
    "    wave_form = load_wav(wav_filename)\n",
    "    feats = extract_features(wave_form)\n",
    "    win_len = 6\n",
    "    overlap = 3\n",
    "    res = extract_overlapping_sections(feats, win_len, overlap)\n",
    "    data = []\n",
    "    times = []\n",
    "    for idx, item in enumerate(res):\n",
    "        start = idx*(overlap/100.)\n",
    "        end = start + (win_len/100.)\n",
    "        data.append(item)\n",
    "        times.append((start, end))\n",
    "    \n",
    "    print(len(data))\n",
    "    result = []\n",
    "    with torch.no_grad():\n",
    "        for idx, d in enumerate(data):\n",
    "            dd = d.flatten().unsqueeze(0)\n",
    "            pred = model(dd)\n",
    "            if pred >= threshold:\n",
    "                result.append(times[idx])\n",
    "    \n",
    "    if len(result)>0:\n",
    "        return merge_and_drop_segments(result, 0.095)\n",
    "    else:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24fc758",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_traced_model = torch.jit.load(\"traced_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a56f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_file(\"data/1693561903662528518.wav\", loaded_traced_model, threshold=0.8888888888888888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5045d40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(29.99 * 1000 / 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1e4657",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.load?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6de2695",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"data/test/1693562020483807935.wav\"\n",
    "waveform, sample_rate = torchaudio.load(fn, normalize=False)\n",
    "waveform, sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5709649",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb2bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#             data = stream.read(self._chunk)\n",
    "#             import numpy as np\n",
    "#             audio_array = np.frombuffer(data, dtype=np.int16)\n",
    "#             print(audio_array.shape)\n",
    "#             audio_array = audio_array.reshape((2,-1), order='F')\n",
    "#             print(audio_array.shape)\n",
    "#             print(audio_array)\n",
    "\n",
    "#             filename = f\"/tmp/sample.wav\"\n",
    "\n",
    "#             print(\"data len\", len(data))\n",
    "#             wf = wave.open(filename, 'wb')\n",
    "#             wf.setnchannels(self._channels)\n",
    "#             wf.setsampwidth(self._sample_size)\n",
    "#             wf.setframerate(self._fs)\n",
    "#             wf.writeframes(data)\n",
    "#             self._logger.info(f\"Stored {filename}\")\n",
    "#             return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48575975",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_len = 6\n",
    "overlap = 3\n",
    "\n",
    "chunk_size = 512\n",
    "clip = []\n",
    "count = 0\n",
    "for i in range(0, waveform.shape[1], chunk_size):\n",
    "    chunk = waveform[:, i:i + chunk_size].flatten() \n",
    "    #chunk = chunk / torch.iinfo(torch.int16).max\n",
    "    # Process the chunk here (replace this with your code)\n",
    "    count += 1\n",
    "    clip.append(chunk)\n",
    "    if len(clip) == 6:\n",
    "        clip = torch.cat(clip, dim=0)\n",
    "        clip = clip.reshape((2,-1))\n",
    "        clip = clip / torch.iinfo(torch.int16).max\n",
    "        print(clip.shape)\n",
    "        mel_spectrogram = torchaudio.compliance.kaldi.fbank(num_mel_bins=80, \n",
    "                                                    frame_length=25, \n",
    "                                                    frame_shift=10,\n",
    "                                                    waveform=clip)\n",
    "        clip = clip.flatten().unsqueeze(0)\n",
    "        print(\"CS\", clip.shape)\n",
    "        pred = model(clip)\n",
    "        if pred >= threshold:\n",
    "            print(pred)\n",
    "        \n",
    "        clip = []\n",
    "        \n",
    "#     stereo_chunk = chunk.reshape((2,-1))\n",
    "#     clip.append(mel_spectrogram)\n",
    "#     if len(clip) == 6:\n",
    "#         clip = torch.cat(clip, dim=0).flatten().unsqueeze(0)\n",
    "#         pred = model(clip)\n",
    "#         if pred >= threshold:\n",
    "#             print(pred)\n",
    "#         clip = []\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd9d8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cea6673",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89743280",
   "metadata": {},
   "outputs": [],
   "source": [
    "int16_to_float32 = T.Resample(orig_freq=44100, new_freq=16000, dtype=torch.float32)\n",
    "int16_to_float32(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02b849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "479818/16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a7078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c6b30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d18026",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e2d8db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
